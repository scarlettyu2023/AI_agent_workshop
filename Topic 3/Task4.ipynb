{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scarlettyu2023/AI_agent_workshop/blob/main/Topic%203/Task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6O_PXzUOH5r"
      },
      "source": [
        "# Learning Goals\n",
        "Know how and why to run an Ollama LLM server\n",
        "\n",
        "Know how to run large commerical models in LangGraph\n",
        "\n",
        "Understand how LLMs call tools using the OpenAI format\n",
        "\n",
        "Understand how to most efficiently define and use LangGraph tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZliAF9s1Oh-C"
      },
      "source": [
        "#1. Set up an Ollama server running Llama 3.2-1B  on either your laptop (if it has a GPU) or on CoLab (if not).  \n",
        "\n",
        "Create two copies of the  llama_mmlu_eval.py code from Topic 1 and modify them so that each runs on a single different topic.  Use the command line time shell function  to measure how long each takes to run in real clock time.  Then modify the programs so they use Ollama.  Start the Ollama server running and then time both sequential and parallel execution of the two programs.  For sequential execution, run\n",
        "\n",
        "time { python program1.py ; python program2.py }\n",
        "Next, time them running in parallel:\n",
        "\n",
        "time { python program1.py & python program2.py & wait; }\n",
        "Write in the README.md file in your portfolio what you observed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJY2-XRCSPB2",
        "outputId": "48ef37ee-75e0-48e9-9313-2d36984361a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  zstd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 116 not upgraded.\n",
            "Need to get 603 kB of archives.\n",
            "After this operation, 1,695 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n",
            "Fetched 603 kB in 1s (509 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
            "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
            "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y zstd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmcXKF9gSuHJ",
        "outputId": "709b78fe-eab5-42d6-9e56-f14c8cd820d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        2511  0.0  0.0   6484  2412 ?        S    18:00   0:00 grep -i ollama\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -e\n",
        "sudo pkill -f \"ollama\" || true\n",
        "sudo fuser -k 11434/tcp || true\n",
        "sleep 1\n",
        "ps aux | grep -i ollama | head -n 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPZ-_T-7Q3kb",
        "outputId": "5a268250-1f9d-4d2d-8c74-f63f6d220ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 121710 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!pip -q install datasets requests tqdm\n",
        "!sudo apt-get -qq update\n",
        "!sudo apt-get -qq install -y pciutils lsb-release curl\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "nohup ollama serve > ollama.log 2>&1 &\n",
        "sleep 2\n",
        "\n",
        "# Check it is running\n",
        "ps aux | grep -i \"ollama serve\" | grep -v grep || true\n",
        "\n",
        "# Check port is open\n",
        "curl -s http://127.0.0.1:11434/api/tags | head -n 5 || true\n",
        "\n",
        "echo \"---- last 30 lines of ollama.log ----\"\n",
        "tail -n 30 ollama.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTWrB2u2qkHi",
        "outputId": "badb9d73-2676-432f-d39e-6d974220a084"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        3720  2.0  0.2 1858408 32264 ?       Sl   18:02   0:00 ollama serve\n",
            "{\"models\":[]}---- last 30 lines of ollama.log ----\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is: \n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBVm6q577PSYiyzy60GAluOUX3l52Un7e6A11sQ3+Qr0\n",
            "\n",
            "time=2026-02-03T18:02:02.406Z level=INFO source=routes.go:1631 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2026-02-03T18:02:02.407Z level=INFO source=images.go:473 msg=\"total blobs: 0\"\n",
            "time=2026-02-03T18:02:02.407Z level=INFO source=images.go:480 msg=\"total unused blobs removed: 0\"\n",
            "time=2026-02-03T18:02:02.407Z level=INFO source=routes.go:1684 msg=\"Listening on 127.0.0.1:11434 (version 0.15.4)\"\n",
            "time=2026-02-03T18:02:02.409Z level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
            "time=2026-02-03T18:02:02.415Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 36067\"\n",
            "time=2026-02-03T18:02:03.820Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 45597\"\n",
            "time=2026-02-03T18:02:04.070Z level=INFO source=runner.go:106 msg=\"experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1\"\n",
            "time=2026-02-03T18:02:04.070Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 42813\"\n",
            "time=2026-02-03T18:02:04.292Z level=INFO source=types.go:42 msg=\"inference compute\" id=GPU-25d0f88f-7d83-59f4-cc31-1151338546b2 filter_id=\"\" library=CUDA compute=7.5 name=CUDA0 description=\"Tesla T4\" libdirs=ollama,cuda_v12 driver=12.4 pci_id=0000:00:04.0 type=discrete total=\"15.0 GiB\" available=\"14.7 GiB\"\n",
            "time=2026-02-03T18:02:04.292Z level=INFO source=routes.go:1725 msg=\"entering low vram mode\" \"total vram\"=\"15.0 GiB\" threshold=\"20.0 GiB\"\n",
            "[GIN] 2026/02/03 - 18:02:04 | 200 |     287.364µs |       127.0.0.1 | GET      \"/api/tags\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezRMFIY5SVDG",
        "outputId": "7610f7e0-8562-47cb-e154-b2d598d352fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"models\":[]}\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "# Start ollama in the background\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# Quick sanity check: server should answer (may take a few seconds right after start)\n",
        "!curl -s http://localhost:11434/api/tags | head\n",
        "\n",
        "# Pull the model (about ~1.3GB)\n",
        "!ollama pull llama3.2:1b\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z05o0imrS2h",
        "outputId": "45e8838e-e232-4f5e-ab41-80ac96adb440"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (1.3.4)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-1.3.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
            "Downloading huggingface_hub-1.3.7-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface_hub 1.3.4\n",
            "    Uninstalling huggingface_hub-1.3.4:\n",
            "      Successfully uninstalled huggingface_hub-1.3.4\n",
            "Successfully installed huggingface_hub-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7f51d7-3ba8-4ea2-88e4-65fa05437378",
        "id": "RgoF22_NrTLa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3.7\n"
          ]
        }
      ],
      "source": [
        "!hf version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqVY5-0jUldu",
        "outputId": "bf5f5d34-1407-43c7-a53e-37692d245bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? [y/N]: y\n",
            "Token is valid (permission: read).\n",
            "The token `scarlettyucs6501` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `scarlettyucs6501`\n"
          ]
        }
      ],
      "source": [
        "!git config --global credential.helper store\n",
        "!hf auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecjrnRt6WCjf",
        "outputId": "673f80c2-dc5f-45a4-acc4-fe678fb388b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ollama.log  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rHRUgg0Vpk7",
        "outputId": "9cde05f3-249e-4c90-ee61-22b2945df0b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Environment Check\n",
            "======================================================================\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âœ“ GPU Available: Tesla T4\n",
            "âœ“ GPU Memory: 15.83 GB\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âš ï¸  Could not check Hugging Face authentication\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cuda\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~2.5 GB (FP16)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cuda\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "Loading weights: 100% 179/179 [00:02<00:00, 64.16it/s, Materializing param=model.norm.weight] \n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cuda:0\n",
            "  Model dtype: torch.float16\n",
            "  GPU Memory: 2.97 GB allocated, 2.97 GB reserved\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: abstract_algebra\n",
            "======================================================================\n",
            "Testing abstract_algebra: 100% 100/100 [00:03<00:00, 32.27it/s]\n",
            "âœ“ Result: 32/100 correct = 32.00%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 100\n",
            "Total Correct: 32\n",
            "Overall Accuracy: 32.00%\n",
            "Duration: 0.1 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260203_182051.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260203_182051.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "\n",
            "======================================================================\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Environment Check\n",
            "======================================================================\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âœ“ GPU Available: Tesla T4\n",
            "âœ“ GPU Memory: 15.83 GB\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âš ï¸  Could not check Hugging Face authentication\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cuda\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~2.5 GB (FP16)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cuda\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "Loading weights: 100% 179/179 [00:02<00:00, 87.57it/s, Materializing param=model.norm.weight] \n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cuda:0\n",
            "  Model dtype: torch.float16\n",
            "  GPU Memory: 2.97 GB allocated, 2.97 GB reserved\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: astronomy\n",
            "======================================================================\n",
            "Testing astronomy: 100% 152/152 [00:05<00:00, 29.95it/s]\n",
            "âœ“ Result: 61/152 correct = 40.13%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 152\n",
            "Total Correct: 61\n",
            "Overall Accuracy: 40.13%\n",
            "Duration: 0.1 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260203_182111.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. astronomy: 40.13%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. astronomy: 40.13%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260203_182111.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "\n",
            "real\t0m42.005s\n",
            "user\t0m29.568s\n",
            "sys\t0m7.523s\n"
          ]
        }
      ],
      "source": [
        "!time { python llama_mmlu_eval_topic1.py ; python llama_mmlu_eval_topic2.py ; }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0yveC3pWbC8",
        "outputId": "aa1a5d62-0be7-4002-8a51-6c515a2d8182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Environment Check\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Environment Check\n",
            "======================================================================\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âœ“ GPU Available: Tesla T4\n",
            "âœ“ GPU Memory: 15.83 GB\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âš ï¸  Could not check Hugging Face authentication\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cuda\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~2.5 GB (FP16)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cuda\n",
            "âœ“ GPU Available: Tesla T4\n",
            "âœ“ GPU Memory: 15.83 GB\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âš ï¸  Could not check Hugging Face authentication\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cuda\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~2.5 GB (FP16)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cuda\n",
            "config.json: 100% 623/623 [00:00<00:00, 3.14MB/s]\n",
            "tokenizer_config.json: 4.34kB [00:00, 2.39MB/s]\n",
            "tokenizer.json: 7.14MB [00:00, 74.3MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 787kB/s]\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "model.safetensors.index.json: 14.9kB [00:00, 49.7MB/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...): 100% 5.94G/5.94G [01:17<00:00, 230MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:17<00:00, 38.99s/it]\n",
            "Download complete: 100% 5.94G/5.94G [01:17<00:00, 230MB/s]                \n",
            "Fetching 2 files: 100% 2/2 [01:17<00:00, 38.99s/it]\n",
            "Download complete: 100% 5.94G/5.94G [01:18<00:00, 76.0MB/s]\n",
            "Download complete: : 0.00B [01:18, ?B/s]\n",
            "Loading weights: 100% 179/179 [00:19<00:00,  8.98it/s, Materializing param=model.norm.weight]\n",
            "\n",
            "generation_config.json: 100% 121/121 [00:00<00:00, 829kB/s]\n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cuda:0\n",
            "  Model dtype: torch.float16\n",
            "  GPU Memory: 2.97 GB allocated, 2.97 GB reserved\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: astronomy\n",
            "======================================================================\n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cuda:0\n",
            "  Model dtype: torch.float16\n",
            "  GPU Memory: 2.97 GB allocated, 2.97 GB reserved\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: abstract_algebra\n",
            "======================================================================\n",
            "README.md: 53.2kB [00:00, 30.5MB/s]\n",
            "dataset_infos.json: 138kB [00:00, 232MB/s]\n",
            "abstract_algebra/test-00000-of-00001.par(…): 100% 9.96k/9.96k [00:00<00:00, 11.6kB/s]\n",
            "astronomy/test-00000-of-00001.parquet: 100% 28.3k/28.3k [00:00<00:00, 28.7kB/s]\n",
            "abstract_algebra/validation-00000-of-000(…): 100% 3.73k/3.73k [00:00<00:00, 10.0kB/s]\n",
            "astronomy/validation-00000-of-00001.parq(…): 100% 6.05k/6.05k [00:00<00:00, 9.03kB/s]\n",
            "abstract_algebra/dev-00000-of-00001.parq(…): 100% 3.45k/3.45k [00:00<00:00, 9.58kB/s]\n",
            "Generating test split: 100% 100/100 [00:00<00:00, 999.56 examples/s]\n",
            "Generating validation split: 100% 11/11 [00:00<00:00, 3648.09 examples/s]\n",
            "Generating dev split: 100% 5/5 [00:00<00:00, 2281.00 examples/s]\n",
            "astronomy/dev-00000-of-00001.parquet: 100% 4.94k/4.94k [00:00<00:00, 7.04kB/s]\n",
            "Generating test split: 100% 152/152 [00:00<00:00, 19587.51 examples/s]\n",
            "Generating validation split: 100% 16/16 [00:00<00:00, 4760.17 examples/s]\n",
            "Generating dev split: 100% 5/5 [00:00<00:00, 1885.93 examples/s]\n",
            "Testing abstract_algebra: 100% 100/100 [00:06<00:00, 16.12it/s]\n",
            "âœ“ Result: 32/100 correct = 32.00%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 100\n",
            "Total Correct: 32\n",
            "Overall Accuracy: 32.00%\n",
            "Duration: 0.2 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260203_182026.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260203_182026.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "Testing astronomy: 100% 152/152 [00:07<00:00, 20.12it/s]\n",
            "âœ“ Result: 61/152 correct = 40.13%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 152\n",
            "Total Correct: 61\n",
            "Overall Accuracy: 40.13%\n",
            "Duration: 0.2 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260203_182028.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. astronomy: 40.13%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. astronomy: 40.13%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260203_182028.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "\n",
            "real\t2m20.943s\n",
            "user\t1m10.920s\n",
            "sys\t0m27.546s\n"
          ]
        }
      ],
      "source": [
        "!time { python llama_mmlu_eval_topic1.py & python llama_mmlu_eval_topic2.py & wait ; }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1TkOUGTqVYy"
      },
      "source": [
        "#2. We will use GPT-4o Mini for the rest of the topic because it is very good at tool use and inexpensive to run (if not completely free) - it should cost less than $1 to complete this topic.  See the full list of OpenAI pricing here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deH02MrbaZaz"
      },
      "source": [
        "Pass the secret key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VhzpZUBqZYw3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "# Make it available like system env vars\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjt-Ums3lO1C",
        "outputId": "c9e21a6a-16ea-4191-e666-8ed7475bbc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working! How can I\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Say: Working!\"}],\n",
        " max_tokens=5)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnB8hW_Zabm0"
      },
      "source": [
        "# Download the code sample for manual tool handling and make sure it runs on your setup.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ICEAzzFa2aO",
        "outputId": "8882c113-39d7-4b0c-90fe-08d1e5cdb4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 1: Query requiring tool\n",
            "============================================================\n",
            "User: What's the weather like in San Francisco?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'San Francisco'}\n",
            "  Result: Sunny, 72Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The weather in San Francisco is sunny, with a temperature of 72°F.\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 2: Query not requiring tool\n",
            "============================================================\n",
            "User: Say hello!\n",
            "\n",
            "--- Iteration 1 ---\n",
            "Assistant: Hello! How can I assist you today?\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 3: Multiple tool calls\n",
            "============================================================\n",
            "User: What's the weather in New York and London?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 2 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'New York'}\n",
            "  Result: Cloudy, 55Â°F\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'London'}\n",
            "  Result: Rainy, 48Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The current weather is as follows:\n",
            "- **New York:** Cloudy, 55°F\n",
            "- **London:** Rainy, 48°F\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python manual_tool_handling.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxeB75xcQZRw"
      },
      "source": [
        "# Modify the code to define a calculator tool that includes geometric functions. Although there are many calculator tools already built into LangGraph, you should define your own from scratch. To parse the input to your tool use json.loads and to format the output from the tool use json.dumps (if needed). For the calculator itself, you could use ast.literal_eval or numexpr. Add examples of the system output to your portfolio. Note: if you find that gpt-4.1-mini insists on trying to do calculations itself instead of using your calculator tool, then explore these strategies for forcing an LLM to use tools and write about it in your README.md."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DhrDGYuTnls",
        "outputId": "3acb99d4-ada4-433e-a7c6-e18c9afc2786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 1: Query requiring tool\n",
            "============================================================\n",
            "User: What's the weather like in San Francisco?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'San Francisco'}\n",
            "  Result: Sunny, 72Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The weather in San Francisco is sunny with a temperature of 72°F.\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 2: Query not requiring tool\n",
            "============================================================\n",
            "User: Say hello!\n",
            "\n",
            "--- Iteration 1 ---\n",
            "Assistant: Hello! How can I assist you today?\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 3: Multiple tool calls\n",
            "============================================================\n",
            "User: What's the weather in New York and London?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 2 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'New York'}\n",
            "  Result: Cloudy, 55Â°F\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'London'}\n",
            "  Result: Rainy, 48Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The current weather is as follows:\n",
            "\n",
            "- **New York**: Cloudy, 55°F\n",
            "- **London**: Rainy, 48°F\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Manual Tool Calling Exercise\n",
        "Students will see how tool calling works under the hood.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# ============================================\n",
        "# PART 1: Define Your Tools\n",
        "# ============================================\n",
        "\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get the current weather for a location\"\"\"\n",
        "    # Simulated weather data\n",
        "    weather_data = {\n",
        "        \"San Francisco\": \"Sunny, 72Â°F\",\n",
        "        \"New York\": \"Cloudy, 55Â°F\",\n",
        "        \"London\": \"Rainy, 48Â°F\",\n",
        "        \"Tokyo\": \"Clear, 65Â°F\"\n",
        "    }\n",
        "    return weather_data.get(location, f\"Weather data not available for {location}\")\n",
        "\n",
        "import ast\n",
        "import math\n",
        "\n",
        "def calculator(operation: str, operands: list) -> str:\n",
        "    \"\"\"\n",
        "    Simple calculator with geometric functions.\n",
        "    Returns JSON string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if operation == \"add\":\n",
        "            result = sum(operands)\n",
        "\n",
        "        elif operation == \"subtract\":\n",
        "            result = operands[0] - operands[1]\n",
        "\n",
        "        elif operation == \"multiply\":\n",
        "            result = operands[0] * operands[1]\n",
        "\n",
        "        elif operation == \"divide\":\n",
        "            result = operands[0] / operands[1]\n",
        "\n",
        "        elif operation == \"area_circle\":\n",
        "            r = operands[0]\n",
        "            result = math.pi * r * r\n",
        "\n",
        "        elif operation == \"circumference_circle\":\n",
        "            r = operands[0]\n",
        "            result = 2 * math.pi * r\n",
        "\n",
        "        elif operation == \"area_rectangle\":\n",
        "            w, h = operands\n",
        "            result = w * h\n",
        "\n",
        "        elif operation == \"hypotenuse\":\n",
        "            a, b = operands\n",
        "            result = math.sqrt(a * a + b * b)\n",
        "\n",
        "        else:\n",
        "            return json.dumps({\"error\": \"Unknown operation\"})\n",
        "\n",
        "        return json.dumps({\"result\": result})\n",
        "\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "# ============================================\n",
        "# PART 2: Describe Tools to the LLM\n",
        "# ============================================\n",
        "\n",
        "# This is the JSON schema that tells the LLM what tools exist\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the current weather for a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city name, e.g. San Francisco\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # TODO: Students will add a second tool here (e.g., calculator)\n",
        "    {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"calculator\",\n",
        "        \"description\": \"Calculator with basic arithmetic and geometric functions\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"operation\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Operation name: add, subtract, multiply, divide, area_circle, circumference_circle, area_rectangle, hypotenuse\"\n",
        "                },\n",
        "                \"operands\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\": \"number\"\n",
        "                    },\n",
        "                    \"description\": \"List of numbers for the operation\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"operation\", \"operands\"]\n",
        "            }\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# PART 3: The Agent Loop\n",
        "# ============================================\n",
        "\n",
        "def run_agent(user_query: str):\n",
        "    \"\"\"\n",
        "    Simple agent that can use tools.\n",
        "    Shows the manual loop that LangGraph automates.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize OpenAI client\n",
        "    client = OpenAI()\n",
        "\n",
        "    # Start conversation with user query\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided tools when needed.\"},\n",
        "        {\"role\": \"user\", \"content\": user_query}\n",
        "    ]\n",
        "\n",
        "    print(f\"User: {user_query}\\n\")\n",
        "\n",
        "    # Agent loop - can iterate up to 5 times\n",
        "    for iteration in range(5):\n",
        "        print(f\"--- Iteration {iteration + 1} ---\")\n",
        "\n",
        "        # Call the LLM\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages,\n",
        "            tools=tools,  # â† This tells the LLM what tools are available\n",
        "            tool_choice=\"auto\"  # Let the model decide whether to use tools\n",
        "        )\n",
        "\n",
        "        assistant_message = response.choices[0].message\n",
        "\n",
        "        # Check if the LLM wants to call a tool\n",
        "        if assistant_message.tool_calls:\n",
        "            print(f\"LLM wants to call {len(assistant_message.tool_calls)} tool(s)\")\n",
        "\n",
        "            # Add the assistant's response to messages\n",
        "            messages.append(assistant_message)\n",
        "\n",
        "            # Execute each tool call\n",
        "            for tool_call in assistant_message.tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "                print(f\"  Tool: {function_name}\")\n",
        "                print(f\"  Args: {function_args}\")\n",
        "\n",
        "                # THIS IS THE MANUAL DISPATCH\n",
        "                # In a real system, you'd use a dictionary lookup\n",
        "                if function_name == \"get_weather\":\n",
        "                    result = get_weather(**function_args)\n",
        "                elif function_name == \"calculator\":\n",
        "                    result = calculator(**function_args)\n",
        "                else:\n",
        "                    result = f\"Error: Unknown function {function_name}\"\n",
        "\n",
        "                print(f\"  Result: {result}\")\n",
        "\n",
        "                # Add the tool result back to the conversation\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": result\n",
        "                })\n",
        "\n",
        "            print()\n",
        "            # Loop continues - LLM will see the tool results\n",
        "\n",
        "        else:\n",
        "            # No tool calls - LLM provided a final answer\n",
        "            print(f\"Assistant: {assistant_message.content}\\n\")\n",
        "            return assistant_message.content\n",
        "\n",
        "    return \"Max iterations reached\"\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# PART 4: Test It\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test query that requires tool use\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEST 1: Query requiring tool\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"What's the weather like in San Francisco?\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 2: Query not requiring tool\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Say hello!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 3: Multiple tool calls\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"What's the weather in New York and London?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAuzRpGgWv7K"
      },
      "source": [
        "# Download the code sample for LangGraph tool handling and make sure it runs on your set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHQ74tDixHAg",
        "outputId": "68a03207-9b18-49ae-bbe9-82a9343c963a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.2.7)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.16.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.6.6)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-1.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7G9cnSDws7z",
        "outputId": "a8af7e6d-19df-4fac-d4aa-3455b63bf5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 1: Query requiring tool\n",
            "============================================================\n",
            "User: What's the weather like in San Francisco?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'San Francisco'}\n",
            "  Result: Sunny, 72Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The weather in San Francisco is sunny with a temperature of 72°F.\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 2: Query not requiring tool\n",
            "============================================================\n",
            "User: Say hello!\n",
            "\n",
            "--- Iteration 1 ---\n",
            "Assistant: Hello! How can I assist you today?\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 3: Multiple tool calls\n",
            "============================================================\n",
            "User: What's the weather in New York and London?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 2 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'New York'}\n",
            "  Result: Cloudy, 55Â°F\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'London'}\n",
            "  Result: Rainy, 48Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The weather is currently as follows:\n",
            "\n",
            "- **New York**: Cloudy, 55°F\n",
            "- **London**: Rainy, 48°F\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python Langgraph_tool_handling.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfYepVHXW1mD"
      },
      "source": [
        "#Add the calculator you created for task 3 to it.  Create a tool that counts the number of occurences of a letter in a piece of text for answering questions like, \"How may s are in Mississippi riverboats?\".  Create a third tool of your own invention.  Record the output as you feed different inputs to the system and save it in your portfolio.  For good style, replace the tool execution dispatch code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qvKGw7CyTBO",
        "outputId": "abaac099-e337-457a-c22d-33bcb4c489bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST 1: Query requiring tool\n",
            "============================================================\n",
            "User: What's the weather like in San Francisco?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'San Francisco'}\n",
            "  Result: Sunny, 72Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The weather in San Francisco is sunny with a temperature of 72°F.\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 2: Query not requiring tool\n",
            "============================================================\n",
            "User: Say hello!\n",
            "\n",
            "--- Iteration 1 ---\n",
            "Assistant: Hello! How can I assist you today?\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 3: Multiple tool calls\n",
            "============================================================\n",
            "User: What's the weather in New York and London?\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 2 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'New York'}\n",
            "  Result: Cloudy, 55Â°F\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'London'}\n",
            "  Result: Rainy, 48Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: The current weather is as follows:\n",
            "- **New York**: Cloudy, 55°F\n",
            "- **London**: Rainy, 48°F\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 4: Multiple tool calls in the same turn (count twice)\n",
            "============================================================\n",
            "User: Are there more i's than s's in 'Mississippi riverboats'? Use tools.\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 2 tool(s)\n",
            "  Tool: count_letter\n",
            "  Args: {'text': 'Mississippi riverboats', 'letter': 'i'}\n",
            "  Result: {\"count\": 5, \"letter\": \"i\", \"case_sensitive\": false}\n",
            "  Tool: count_letter\n",
            "  Args: {'text': 'Mississippi riverboats', 'letter': 's'}\n",
            "  Result: {\"count\": 5, \"letter\": \"s\", \"case_sensitive\": false}\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Assistant: In the phrase \"Mississippi riverboats,\" there are 5 occurrences of the letter 'i' and 5 occurrences of the letter 's'. Therefore, there are not more 'i's than 's's; they are equal.\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 5: Sequential chaining (count twice → calculator next outer iteration)\n",
            "============================================================\n",
            "User: What is the sin of the difference between the number of i's and the number of s's in 'Mississippi riverboats'? Use tools.\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 2 tool(s)\n",
            "  Tool: count_letter\n",
            "  Args: {'text': 'Mississippi riverboats', 'letter': 'i'}\n",
            "  Result: {\"count\": 5, \"letter\": \"i\", \"case_sensitive\": false}\n",
            "  Tool: count_letter\n",
            "  Args: {'text': 'Mississippi riverboats', 'letter': 's'}\n",
            "  Result: {\"count\": 5, \"letter\": \"s\", \"case_sensitive\": false}\n",
            "\n",
            "--- Iteration 2 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'sin', 'operands': [0]}\n",
            "  Result: {\"error\": \"Unknown operation\"}\n",
            "\n",
            "--- Iteration 3 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'subtract', 'operands': [5, 5]}\n",
            "  Result: {\"result\": 0}\n",
            "\n",
            "--- Iteration 4 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'sin', 'operands': [0]}\n",
            "  Result: {\"error\": \"Unknown operation\"}\n",
            "\n",
            "--- Iteration 5 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'sin', 'operands': [0.0]}\n",
            "  Result: {\"error\": \"Unknown operation\"}\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 6: Use all tools in one query\n",
            "============================================================\n",
            "User: Using tools: (1) Get the weather in Tokyo. (2) Count the number of 's' in 'Mississippi riverboats'. (3) Compute text metrics for 'Mississippi riverboats'. (4) Compute the circumference of a circle with radius = count_s. Summarize everything.\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 3 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'Tokyo'}\n",
            "  Result: Clear, 65Â°F\n",
            "  Tool: count_letter\n",
            "  Args: {'text': 'Mississippi riverboats', 'letter': 's'}\n",
            "  Result: {\"count\": 5, \"letter\": \"s\", \"case_sensitive\": false}\n",
            "  Tool: text_metrics\n",
            "  Args: {'text': 'Mississippi riverboats'}\n",
            "  Result: {\"length_chars\": 22, \"word_count\": 2, \"unique_letter_count\": 11, \"unique_letters\": [\"a\", \"b\", \"e\", \"i\", \"m\", \"o\", \"p\", \"r\", \"s\", \"t\", \"v\"]}\n",
            "\n",
            "--- Iteration 2 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'circumference', 'operands': [5]}\n",
            "  Result: {\"error\": \"Unknown operation\"}\n",
            "\n",
            "--- Iteration 3 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'circumference', 'operands': [5]}\n",
            "  Result: {\"error\": \"Unknown operation\"}\n",
            "\n",
            "--- Iteration 4 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'circle_circumference', 'operands': [5]}\n",
            "  Result: {\"error\": \"Unknown operation\"}\n",
            "\n",
            "--- Iteration 5 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'multiply', 'operands': [2, 3.14159]}\n",
            "  Result: {\"result\": 6.28318}\n",
            "\n",
            "\n",
            "============================================================\n",
            "TEST 7: Try to hit the 5-iteration limit\n",
            "============================================================\n",
            "User: Call at most one tool per turn. Step 1: weather in London. Step 2: text_metrics for 'Mississippi riverboats'. Step 3: count 'i'. Step 4: count 's'. Step 5: compute sin(i-s). After each step, state what you learned and what you will do next.\n",
            "\n",
            "--- Iteration 1 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: get_weather\n",
            "  Args: {'location': 'London'}\n",
            "  Result: Rainy, 48Â°F\n",
            "\n",
            "--- Iteration 2 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: text_metrics\n",
            "  Args: {'text': 'Mississippi riverboats'}\n",
            "  Result: {\"length_chars\": 22, \"word_count\": 2, \"unique_letter_count\": 11, \"unique_letters\": [\"a\", \"b\", \"e\", \"i\", \"m\", \"o\", \"p\", \"r\", \"s\", \"t\", \"v\"]}\n",
            "\n",
            "--- Iteration 3 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: count_letter\n",
            "  Args: {'text': 'Mississippi riverboats', 'letter': 'i'}\n",
            "  Result: {\"count\": 5, \"letter\": \"i\", \"case_sensitive\": false}\n",
            "\n",
            "--- Iteration 4 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: count_letter\n",
            "  Args: {'text': 'Mississippi riverboats', 'letter': 's'}\n",
            "  Result: {\"count\": 5, \"letter\": \"s\", \"case_sensitive\": false}\n",
            "\n",
            "--- Iteration 5 ---\n",
            "LLM wants to call 1 tool(s)\n",
            "  Tool: calculator\n",
            "  Args: {'operation': 'sin', 'operands': [0]}\n",
            "  Result: {\"error\": \"Unknown operation\"}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Tool Calling with LangChain\n",
        "Shows how LangChain abstracts tool calling.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
        "import math\n",
        "import json\n",
        "\n",
        "# ============================================\n",
        "# PART 1: Define Your Tools\n",
        "# ============================================\n",
        "\n",
        "@tool\n",
        "def get_weather(location: str) -> str:\n",
        "    \"\"\"Get the current weather for a given location\"\"\"\n",
        "    # Simulated weather data\n",
        "    weather_data = {\n",
        "        \"San Francisco\": \"Sunny, 72Â°F\",\n",
        "        \"New York\": \"Cloudy, 55Â°F\",\n",
        "        \"London\": \"Rainy, 48Â°F\",\n",
        "        \"Tokyo\": \"Clear, 65Â°F\"\n",
        "    }\n",
        "    return weather_data.get(location, f\"Weather data not available for {location}\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def calculator(operation: str, operands: list) -> str:\n",
        "    \"\"\"\n",
        "    Simple calculator with geometric functions.\n",
        "    Returns JSON string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if operation == \"add\":\n",
        "            result = sum(operands)\n",
        "\n",
        "        elif operation == \"subtract\":\n",
        "            result = operands[0] - operands[1]\n",
        "\n",
        "        elif operation == \"multiply\":\n",
        "            result = operands[0] * operands[1]\n",
        "\n",
        "        elif operation == \"divide\":\n",
        "            result = operands[0] / operands[1]\n",
        "\n",
        "        elif operation == \"area_circle\":\n",
        "            r = operands[0]\n",
        "            result = math.pi * r * r\n",
        "\n",
        "        elif operation == \"circumference_circle\":\n",
        "            r = operands[0]\n",
        "            result = 2 * math.pi * r\n",
        "\n",
        "        elif operation == \"area_rectangle\":\n",
        "            w, h = operands\n",
        "            result = w * h\n",
        "\n",
        "        elif operation == \"hypotenuse\":\n",
        "            a, b = operands\n",
        "            result = math.sqrt(a * a + b * b)\n",
        "\n",
        "        else:\n",
        "            return json.dumps({\"error\": \"Unknown operation\"})\n",
        "\n",
        "        return json.dumps({\"result\": result})\n",
        "\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "\n",
        "@tool\n",
        "def count_letter(text: str, letter: str, case_sensitive: bool = False) -> str:\n",
        "    \"\"\"Count occurrences of a letter in text. Returns JSON string.\"\"\"\n",
        "    try:\n",
        "        if not letter or len(letter) != 1:\n",
        "            return json.dumps({\"error\": \"letter must be a single character\"})\n",
        "\n",
        "        if case_sensitive:\n",
        "            c = text.count(letter)\n",
        "        else:\n",
        "            c = text.lower().count(letter.lower())\n",
        "\n",
        "        return json.dumps({\"count\": c, \"letter\": letter, \"case_sensitive\": case_sensitive})\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "\n",
        "@tool\n",
        "def text_metrics(text: str) -> str:\n",
        "    \"\"\"Third tool (custom): basic text stats. Returns JSON string.\"\"\"\n",
        "    try:\n",
        "        length_chars = len(text)\n",
        "        word_count = len([w for w in text.split() if w.strip()])\n",
        "        unique_letters = sorted({ch.lower() for ch in text if ch.isalpha()})\n",
        "        return json.dumps({\n",
        "            \"length_chars\": length_chars,\n",
        "            \"word_count\": word_count,\n",
        "            \"unique_letter_count\": len(unique_letters),\n",
        "            \"unique_letters\": unique_letters\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# PART 2: Create LLM with Tools\n",
        "# ============================================\n",
        "\n",
        "# Create LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Bind tools to LLM\n",
        "tools = [get_weather, calculator, count_letter, text_metrics]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "tool_map = {t.name: t for t in tools}\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# PART 3: The Agent Loop\n",
        "# ============================================\n",
        "\n",
        "def run_agent(user_query: str):\n",
        "    \"\"\"\n",
        "    Simple agent that can use tools.\n",
        "    Shows the manual loop that LangGraph automates.\n",
        "    \"\"\"\n",
        "\n",
        "    # Start conversation with user query\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful assistant. Use the provided tools when needed.\"),\n",
        "        HumanMessage(content=user_query)\n",
        "    ]\n",
        "\n",
        "    print(f\"User: {user_query}\\n\")\n",
        "\n",
        "    # Agent loop - can iterate up to 5 times\n",
        "    for iteration in range(5):\n",
        "        print(f\"--- Iteration {iteration + 1} ---\")\n",
        "\n",
        "        # Call the LLM\n",
        "        response = llm_with_tools.invoke(messages)\n",
        "\n",
        "        # Check if the LLM wants to call a tool\n",
        "        if response.tool_calls:\n",
        "            print(f\"LLM wants to call {len(response.tool_calls)} tool(s)\")\n",
        "\n",
        "            # Add the assistant's response to messages\n",
        "            messages.append(response)\n",
        "\n",
        "            # Execute each tool call\n",
        "            for tool_call in response.tool_calls:\n",
        "                function_name = tool_call[\"name\"]\n",
        "                function_args = tool_call[\"args\"]\n",
        "\n",
        "                print(f\"  Tool: {function_name}\")\n",
        "                print(f\"  Args: {function_args}\")\n",
        "\n",
        "                # Execute the tool\n",
        "                if function_name in tool_map:\n",
        "                    result = tool_map[function_name].invoke(function_args)\n",
        "                else:\n",
        "                    result = f\"Error: Unknown function {function_name}\"\n",
        "\n",
        "                print(f\"  Result: {result}\")\n",
        "                # Add the tool result back to the conversation\n",
        "                messages.append(ToolMessage(\n",
        "                    content=result,\n",
        "                    tool_call_id=tool_call[\"id\"]\n",
        "                ))\n",
        "\n",
        "            print()\n",
        "            # Loop continues - LLM will see the tool results\n",
        "\n",
        "        else:\n",
        "            # No tool calls - LLM provided a final answer\n",
        "            print(f\"Assistant: {response.content}\\n\")\n",
        "            return response.content\n",
        "\n",
        "    return \"Max iterations reached\"\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# PART 4: Test It\n",
        "# ============================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test query that requires tool use\n",
        "    print(\"=\"*60)\n",
        "    print(\"TEST 1: Query requiring tool\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"What's the weather like in San Francisco?\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 2: Query not requiring tool\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Say hello!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 3: Multiple tool calls\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"What's the weather in New York and London?\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 4: Multiple tool calls in the same turn (count twice)\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Are there more i's than s's in 'Mississippi riverboats'? Use tools.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 5: Sequential chaining (count twice → calculator next outer iteration)\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"What is the sin of the difference between the number of i's and the number of s's in 'Mississippi riverboats'? Use tools.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 6: Use all tools in one query\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Using tools: (1) Get the weather in Tokyo. (2) Count the number of 's' in 'Mississippi riverboats'. (3) Compute text metrics for 'Mississippi riverboats'. (4) Compute the circumference of a circle with radius = count_s. Summarize everything.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST 7: Try to hit the 5-iteration limit\")\n",
        "    print(\"=\"*60)\n",
        "    run_agent(\"Call at most one tool per turn. Step 1: weather in London. Step 2: text_metrics for 'Mississippi riverboats'. Step 3: count 'i'. Step 4: count 's'. Step 5: compute sin(i-s). After each step, state what you learned and what you will do next.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78Xmesv0xYQs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOzCXo/joKdkIZ6D1dj8NsD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}