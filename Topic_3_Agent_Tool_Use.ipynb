{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9ufBlwP1vsNw5KUARZUHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scarlettyu2023/AI_agent_workshop/blob/main/Topic_3_Agent_Tool_Use.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Goals\n",
        "Know how and why to run an Ollama LLM server\n",
        "\n",
        "Know how to run large commerical models in LangGraph\n",
        "\n",
        "Understand how LLMs call tools using the OpenAI format\n",
        "\n",
        "Understand how to most efficiently define and use LangGraph tools"
      ],
      "metadata": {
        "id": "e6O_PXzUOH5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Set up an Ollama server running Llama 3.2-1B  on either your laptop (if it has a GPU) or on CoLab (if not).  \n",
        "\n",
        "Create two copies of the  llama_mmlu_eval.py code from Topic 1 and modify them so that each runs on a single different topic.  Use the command line time shell function  to measure how long each takes to run in real clock time.  Then modify the programs so they use Ollama.  Start the Ollama server running and then time both sequential and parallel execution of the two programs.  For sequential execution, run\n",
        "\n",
        "time { python program1.py ; python program2.py }\n",
        "Next, time them running in parallel:\n",
        "\n",
        "time { python program1.py & python program2.py & wait; }\n",
        "Write in the README.md file in your portfolio what you observed.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZliAF9s1Oh-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y zstd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJY2-XRCSPB2",
        "outputId": "c1c72174-65a8-48a0-f74e-866791c9af00"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  zstd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 603 kB of archives.\n",
            "After this operation, 1,695 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n",
            "Fetched 603 kB in 2s (341 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 117562 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
            "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
            "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "sudo pkill -f \"ollama\" || true\n",
        "sudo fuser -k 11434/tcp || true\n",
        "sleep 1\n",
        "ps aux | grep -i ollama | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmcXKF9gSuHJ",
        "outputId": "420b50e6-35ca-4752-c1cb-018560ebcd0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        6420  0.0  0.0   6480  2516 ?        S    19:46   0:00 grep -i ollama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "nohup ollama serve > ollama.log 2>&1 &\n",
        "sleep 2\n",
        "\n",
        "# Check it is running\n",
        "ps aux | grep -i \"ollama serve\" | grep -v grep || true\n",
        "\n",
        "# Check port is open\n",
        "curl -s http://127.0.0.1:11434/api/tags | head -n 5 || true\n",
        "\n",
        "echo \"---- last 30 lines of ollama.log ----\"\n",
        "tail -n 30 ollama.log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_unPCYXSzmR",
        "outputId": "fbbd9bd9-f0c9-4236-dab4-2534f41ee644"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        6507  1.0  0.2 1784652 32292 ?       Sl   19:46   0:00 ollama serve\n",
            "{\"models\":[]}---- last 30 lines of ollama.log ----\n",
            "time=2026-01-29T19:46:47.497Z level=INFO source=routes.go:1631 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2026-01-29T19:46:47.497Z level=INFO source=images.go:473 msg=\"total blobs: 0\"\n",
            "time=2026-01-29T19:46:47.497Z level=INFO source=images.go:480 msg=\"total unused blobs removed: 0\"\n",
            "time=2026-01-29T19:46:47.497Z level=INFO source=routes.go:1684 msg=\"Listening on 127.0.0.1:11434 (version 0.15.2)\"\n",
            "time=2026-01-29T19:46:47.498Z level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
            "time=2026-01-29T19:46:47.498Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 37131\"\n",
            "time=2026-01-29T19:46:47.542Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 45115\"\n",
            "time=2026-01-29T19:46:47.589Z level=INFO source=runner.go:106 msg=\"experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1\"\n",
            "time=2026-01-29T19:46:47.589Z level=INFO source=types.go:60 msg=\"inference compute\" id=cpu library=cpu compute=\"\" name=cpu description=cpu libdirs=ollama driver=\"\" pci_id=\"\" type=\"\" total=\"12.7 GiB\" available=\"11.1 GiB\"\n",
            "time=2026-01-29T19:46:47.589Z level=INFO source=routes.go:1725 msg=\"entering low vram mode\" \"total vram\"=\"0 B\" threshold=\"20.0 GiB\"\n",
            "[GIN] 2026/01/29 - 19:46:49 | 200 |     494.612µs |       127.0.0.1 | GET      \"/api/tags\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets requests tqdm\n",
        "!sudo apt-get -qq update\n",
        "!sudo apt-get -qq install -y pciutils lsb-release curl\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ],
      "metadata": {
        "id": "gPZ-_T-7Q3kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4870f0f5-2fdc-4ce9-9614-cbb08a5a8663"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start ollama in the background\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "\n",
        "# Quick sanity check: server should answer (may take a few seconds right after start)\n",
        "!curl -s http://localhost:11434/api/tags | head\n",
        "\n",
        "# Pull the model (about ~1.3GB)\n",
        "!ollama pull llama3.2:1b\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezRMFIY5SVDG",
        "outputId": "baa3bc52-365c-4f59-9c97-d1946d84814c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"models\":[]}\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hf version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y_f5uhUUdTI",
        "outputId": "d3a6a547-f9e5-4688-f8b3-1da008ed8697"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface_hub version: 0.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store\n",
        "!hf auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqVY5-0jUldu",
        "outputId": "5c15dedd-f870-4e0c-9553-c889b44225c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "The token `scarlettyucs6501` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `scarlettyucs6501`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecjrnRt6WCjf",
        "outputId": "c6376d04-7b89-42e4-b893-ff9e39020b78"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama_mmlu_eval.py\t   llama_mmlu_eval_topic2.py  sample_data\n",
            "llama_mmlu_eval_topic1.py  ollama.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!time { python llama_mmlu_eval_topic1.py ; python llama_mmlu_eval_topic2.py ; }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rHRUgg0Vpk7",
        "outputId": "e3b593c0-1c2c-46fe-86d2-2ce39caab7f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Environment Check\n",
            "======================================================================\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âš ï¸  No GPU detected - running on CPU\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âœ“ Hugging Face authenticated\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cpu\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~5 GB (FP32)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cpu\n",
            "tokenizer_config.json: 4.34kB [00:00, 5.33MB/s]\n",
            "vocab.json: 1.61MB [00:00, 9.05MB/s]\n",
            "merges.txt: 917kB [00:00, 5.73MB/s]\n",
            "tokenizer.json: 7.14MB [00:00, 18.3MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 853kB/s]\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "config.json: 100% 623/623 [00:00<00:00, 4.31MB/s]\n",
            "2026-01-29 20:02:04.853824: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-29 20:02:05.279146: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-29 20:02:05.471335: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769716925.725932   10857 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769716925.803039   10857 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769716926.307033   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769716926.307079   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769716926.307085   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769716926.307091   10857 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-29 20:02:06.357447: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "model.safetensors.index.json: 14.9kB [00:00, 41.1MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/956M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 628k/4.98G [00:03<7:01:22, 197kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 17.7M/956M [00:04<03:48, 4.11MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 1.94M/4.98G [00:04<3:09:12, 439kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 84.8M/956M [00:06<01:02, 14.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 152M/956M [00:07<00:28, 28.2MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 219M/956M [00:07<00:16, 43.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 286M/956M [00:10<00:20, 32.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 353M/956M [00:11<00:13, 46.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 69.0M/4.98G [00:11<10:38, 7.69MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 420M/956M [00:11<00:09, 56.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 487M/956M [00:12<00:06, 70.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 554M/956M [00:12<00:04, 84.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.98G [00:13<06:04, 13.3MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 621M/956M [00:13<00:04, 71.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 203M/4.98G [00:13<03:31, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 270M/4.98G [00:14<02:18, 34.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 688M/956M [00:14<00:03, 72.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 755M/956M [00:15<00:02, 84.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 822M/956M [00:15<00:01, 91.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 337M/4.98G [00:16<02:28, 31.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 406M/4.98G [00:18<02:09, 35.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 474M/4.98G [00:18<01:39, 45.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 889M/956M [00:20<00:01, 35.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 541M/4.98G [00:20<01:35, 46.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 956M/956M [00:21<00:00, 42.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 956M/956M [00:21<00:00, 44.8MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 675M/4.98G [00:21<01:04, 67.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 745M/4.98G [00:22<00:54, 78.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 812M/4.98G [00:22<00:41, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 879M/4.98G [00:22<00:31, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 946M/4.98G [00:22<00:25, 156MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.98G [00:22<00:23, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.98G [00:23<00:24, 158MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.98G [00:23<00:21, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.98G [00:23<00:18, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.98G [00:24<00:15, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.98G [00:25<00:31, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.98G [00:25<00:24, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.98G [00:25<00:22, 158MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.98G [00:30<01:30, 38.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 1.62G/4.98G [00:31<01:14, 45.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.98G [00:31<00:40, 79.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 1.82G/4.98G [00:32<00:32, 96.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.98G [00:32<00:29, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.98G [00:33<00:29, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.98G [00:34<00:38, 77.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.98G [00:34<00:29, 97.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.98G [00:36<00:46, 61.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.98G [00:37<00:33, 82.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.98G [00:37<00:27, 98.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.98G [00:38<00:24, 105MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.98G [00:38<00:18, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.98G [00:38<00:15, 162MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.98G [00:41<00:39, 60.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.98G [00:41<00:29, 79.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.98G [00:41<00:21, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.98G [00:42<00:21, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.98G [00:47<00:59, 36.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.98G [00:47<00:33, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.98G [00:51<00:48, 40.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/4.98G [00:51<00:37, 50.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.98G [00:51<00:27, 66.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/4.98G [00:51<00:20, 83.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.31G/4.98G [00:52<00:16, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.98G [00:52<00:13, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.98G [00:52<00:10, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 3.51G/4.98G [00:53<00:13, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.98G [00:57<00:33, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 3.71G/4.98G [00:58<00:19, 66.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 3.78G/4.98G [00:58<00:15, 78.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.98G [00:58<00:12, 92.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.98G [00:59<00:10, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.98G [01:01<00:16, 61.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.98G [01:01<00:08, 99.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.98G [01:02<00:06, 119MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.98G [01:02<00:05, 140MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.98G [01:02<00:04, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.98G [01:02<00:03, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.98G [01:03<00:02, 180MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.98G [01:03<00:02, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.98G [01:03<00:01, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.98G [01:04<00:01, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.98G [01:04<00:01, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.98G [01:04<00:00, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 4.85G/4.98G [01:04<00:00, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 4.98G/4.98G [01:05<00:00, 76.4MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:06<00:00, 33.06s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 21.25it/s]\n",
            "generation_config.json: 100% 121/121 [00:00<00:00, 605kB/s]\n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cpu\n",
            "  Model dtype: torch.float32\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: abstract_algebra\n",
            "======================================================================\n",
            "README.md: 53.2kB [00:00, 56.8MB/s]\n",
            "dataset_infos.json: 138kB [00:00, 243MB/s]\n",
            "abstract_algebra/test-00000-of-00001.par(…): 100% 9.96k/9.96k [00:00<00:00, 19.8kB/s]\n",
            "abstract_algebra/validation-00000-of-000(…): 100% 3.73k/3.73k [00:00<00:00, 7.43kB/s]\n",
            "abstract_algebra/dev-00000-of-00001.parq(…): 100% 3.45k/3.45k [00:00<00:00, 7.22kB/s]\n",
            "Generating test split: 100% 100/100 [00:00<00:00, 1292.72 examples/s]\n",
            "Generating validation split: 100% 11/11 [00:00<00:00, 3701.35 examples/s]\n",
            "Generating dev split: 100% 5/5 [00:00<00:00, 1884.74 examples/s]\n",
            "Testing abstract_algebra: 100% 100/100 [06:24<00:00,  3.84s/it]\n",
            "âœ“ Result: 32/100 correct = 32.00%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 100\n",
            "Total Correct: 32\n",
            "Overall Accuracy: 32.00%\n",
            "Duration: 6.5 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260129_200956.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260129_200956.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "\n",
            "======================================================================\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Environment Check\n",
            "======================================================================\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âš ï¸  No GPU detected - running on CPU\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âœ“ Hugging Face authenticated\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cpu\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~5 GB (FP32)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cpu\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "2026-01-29 20:10:11.924332: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-29 20:10:11.992773: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-29 20:10:12.043144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769717412.108496   12900 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769717412.125659   12900 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769717412.176158   12900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769717412.176213   12900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769717412.176223   12900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769717412.176232   12900 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-29 20:10:12.185048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 79.13it/s]\n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cpu\n",
            "  Model dtype: torch.float32\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: astronomy\n",
            "======================================================================\n",
            "astronomy/test-00000-of-00001.parquet: 100% 28.3k/28.3k [00:01<00:00, 15.7kB/s]\n",
            "astronomy/validation-00000-of-00001.parq(…): 100% 6.05k/6.05k [00:01<00:00, 4.41kB/s]\n",
            "astronomy/dev-00000-of-00001.parquet: 100% 4.94k/4.94k [00:00<00:00, 6.04kB/s]\n",
            "Generating test split: 100% 152/152 [00:00<00:00, 23581.81 examples/s]\n",
            "Generating validation split: 100% 16/16 [00:00<00:00, 5337.11 examples/s]\n",
            "Generating dev split: 100% 5/5 [00:00<00:00, 1908.58 examples/s]\n",
            "Testing astronomy: 100% 152/152 [10:26<00:00,  4.12s/it]\n",
            "âœ“ Result: 60/152 correct = 39.47%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 152\n",
            "Total Correct: 60\n",
            "Overall Accuracy: 39.47%\n",
            "Duration: 10.6 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260129_202056.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. astronomy: 39.47%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. astronomy: 39.47%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260129_202056.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "\n",
            "real\t19m22.641s\n",
            "user\t17m29.604s\n",
            "sys\t0m33.338s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!time { python llama_mmlu_eval_topic1.py & python llama_mmlu_eval_topic2.py & wait ; }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0yveC3pWbC8",
        "outputId": "068550a2-e33f-48ed-886c-d9d40bef715c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "======================================================================\n",
            "\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "======================================================================\n",
            "\n",
            "Environment Check\n",
            "======================================================================\n",
            "======================================================================\n",
            "Environment Check\n",
            "======================================================================\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âœ“ Running in Google Colab\n",
            "âœ“ Platform: Linux (x86_64)\n",
            "âœ“ GPU Available: Tesla T4\n",
            "âœ“ GPU Memory: 15.83 GB\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âš ï¸  No Hugging Face token found\n",
            "Run: huggingface-cli login\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cuda\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~2.5 GB (FP16)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cuda\n",
            "âœ“ GPU Available: Tesla T4\n",
            "âœ“ GPU Memory: 15.83 GB\n",
            "âœ“ Quantization disabled - loading full precision model\n",
            "âš ï¸  No Hugging Face token found\n",
            "Run: huggingface-cli login\n",
            "\n",
            "======================================================================\n",
            "Configuration\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "Device: cuda\n",
            "Quantization: None (full precision)\n",
            "Expected memory: ~2.5 GB (FP16)\n",
            "Number of subjects: 1\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Loading model allenai/OLMo-2-0425-1B...\n",
            "Device: cuda\n",
            "tokenizer_config.json: 4.34kB [00:00, 15.9MB/s]\n",
            "vocab.json: 1.61MB [00:00, 107MB/s]\n",
            "merges.txt: 917kB [00:00, 119MB/s]\n",
            "tokenizer.json: 7.14MB [00:00, 191MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 1.31MB/s]\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "âœ“ Tokenizer loaded\n",
            "Loading model (this may take 2-3 minutes)...\n",
            "config.json: 100% 623/623 [00:00<00:00, 5.67MB/s]\n",
            "2026-01-29 20:50:19.141256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2026-01-29 20:50:19.141255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769719819.173474    6821 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769719819.173474    6822 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769719819.182721    6822 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "E0000 00:00:1769719819.182725    6821 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769719819.203714    6822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769719819.203714    6821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769719819.203739    6821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769719819.203749    6821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769719819.203744    6822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769719819.203756    6821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769719819.203770    6822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769719819.203778    6822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-29 20:50:19.209718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-01-29 20:50:19.209719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors.index.json: 14.9kB [00:00, 40.9MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 0.00/956M [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 628k/4.98G [00:03<7:49:05, 177kB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 17.7M/956M [00:04<03:38, 4.30MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 84.7M/956M [00:04<00:38, 22.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 1.94M/4.98G [00:04<2:58:15, 466kB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 152M/956M [00:05<00:19, 42.0MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 69.0M/4.98G [00:05<03:57, 20.7MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 219M/956M [00:05<00:12, 58.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 286M/956M [00:06<00:09, 71.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.98G [00:06<02:35, 31.2MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 353M/956M [00:06<00:07, 83.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 420M/956M [00:07<00:05, 105MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 487M/956M [00:07<00:04, 107MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 554M/956M [00:08<00:03, 121MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 205M/4.98G [00:08<02:12, 35.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 621M/956M [00:09<00:03, 89.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 688M/956M [00:09<00:02, 102MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 272M/4.98G [00:12<02:58, 26.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 755M/956M [00:12<00:03, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 339M/4.98G [00:12<01:57, 39.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 822M/956M [00:12<00:01, 75.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 889M/956M [00:12<00:00, 92.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 956M/956M [00:13<00:00, 73.3MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 406M/4.98G [00:13<01:47, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 473M/4.98G [00:14<01:23, 54.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 541M/4.98G [00:14<00:58, 75.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.98G [00:14<00:45, 95.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 675M/4.98G [00:14<00:37, 115MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 742M/4.98G [00:15<00:30, 138MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 809M/4.98G [00:15<00:25, 161MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 876M/4.98G [00:15<00:24, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 943M/4.98G [00:15<00:19, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.98G [00:18<00:53, 73.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.98G [00:18<00:32, 119MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.98G [00:18<00:26, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.98G [00:19<00:27, 136MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.98G [00:19<00:22, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.98G [00:19<00:18, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.98G [00:20<00:22, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.98G [00:20<00:11, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.98G [00:20<00:13, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.82G/4.98G [00:21<00:13, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.98G [00:21<00:13, 233MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.98G [00:22<00:17, 173MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.02G/4.98G [00:22<00:15, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.98G [00:22<00:12, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.98G [00:26<00:54, 52.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.98G [00:27<00:41, 65.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.98G [00:30<00:56, 46.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.48G/4.98G [00:30<00:34, 72.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.98G [00:34<01:10, 35.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.98G [00:35<00:54, 44.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.98G [00:35<00:39, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.98G [00:35<00:31, 72.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.98G [00:36<00:23, 94.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.98G [00:36<00:17, 121MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.91G/4.98G [00:36<00:15, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.98G [00:36<00:12, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.98G [00:37<00:10, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/4.98G [00:37<00:09, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.98G [00:37<00:08, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.98G [00:37<00:07, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.31G/4.98G [00:37<00:06, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.98G [00:38<00:05, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.98G [00:38<00:06, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.51G/4.98G [00:40<00:18, 79.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.98G [00:40<00:09, 135MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.71G/4.98G [00:41<00:08, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.78G/4.98G [00:41<00:07, 155MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.98G [00:41<00:05, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.98G [00:44<00:13, 72.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.98G [00:45<00:10, 88.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.98G [00:45<00:08, 105MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.98G [00:45<00:06, 126MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.98G [00:45<00:04, 152MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.98G [00:46<00:03, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.98G [00:46<00:03, 188MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.98G [00:46<00:02, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.98G [00:46<00:01, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.98G [00:47<00:01, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.98G [00:47<00:01, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.98G [00:47<00:01, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.98G [00:47<00:00, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.85G/4.98G [00:48<00:00, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.98G [00:48<00:00, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.98G/4.98G [00:48<00:00, 103MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:49<00:00, 24.60s/it]\n",
            "Fetching 2 files: 100% 2/2 [00:49<00:00, 24.61s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:19<00:00,  9.52s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:19<00:00,  9.54s/it]\n",
            "generation_config.json: 100% 121/121 [00:00<00:00, 1.29MB/s]\n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cuda:0\n",
            "  Model dtype: torch.float16\n",
            "  GPU Memory: 2.97 GB allocated, 3.79 GB reserved\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: abstract_algebra\n",
            "======================================================================\n",
            "âœ“ Model loaded successfully!\n",
            "  Model device: cuda:0\n",
            "  Model dtype: torch.float16\n",
            "  GPU Memory: 2.97 GB allocated, 3.79 GB reserved\n",
            "\n",
            "======================================================================\n",
            "Starting evaluation on 1 subjects\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Progress: 1/1 subjects\n",
            "\n",
            "======================================================================\n",
            "Evaluating subject: astronomy\n",
            "======================================================================\n",
            "README.md: 53.2kB [00:00, 91.3MB/s]\n",
            "dataset_infos.json: 138kB [00:00, 214MB/s]\n",
            "abstract_algebra/test-00000-of-00001.par(…): 100% 9.96k/9.96k [00:01<00:00, 8.83kB/s]\n",
            "astronomy/test-00000-of-00001.parquet: 100% 28.3k/28.3k [00:01<00:00, 16.2kB/s]\n",
            "abstract_algebra/validation-00000-of-000(…): 100% 3.73k/3.73k [00:00<00:00, 4.25kB/s]\n",
            "astronomy/validation-00000-of-00001.parq(…): 100% 6.05k/6.05k [00:00<00:00, 6.70kB/s]\n",
            "abstract_algebra/dev-00000-of-00001.parq(…): 100% 3.45k/3.45k [00:00<00:00, 3.99kB/s]\n",
            "Generating test split: 100% 100/100 [00:00<00:00, 1008.21 examples/s]\n",
            "Generating validation split: 100% 11/11 [00:00<00:00, 4592.61 examples/s]\n",
            "Generating dev split: 100% 5/5 [00:00<00:00, 2536.78 examples/s]\n",
            "astronomy/dev-00000-of-00001.parquet: 100% 4.94k/4.94k [00:00<00:00, 6.78kB/s]\n",
            "Generating test split: 100% 152/152 [00:00<00:00, 31185.94 examples/s]\n",
            "Generating validation split: 100% 16/16 [00:00<00:00, 6799.28 examples/s]\n",
            "Generating dev split: 100% 5/5 [00:00<00:00, 2354.76 examples/s]\n",
            "Testing abstract_algebra: 100% 100/100 [00:06<00:00, 16.17it/s]\n",
            "âœ“ Result: 32/100 correct = 32.00%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 100\n",
            "Total Correct: 32\n",
            "Overall Accuracy: 32.00%\n",
            "Duration: 0.2 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260129_205152.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. abstract_algebra: 32.00%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260129_205152.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "Testing astronomy: 100% 152/152 [00:07<00:00, 19.65it/s]\n",
            "âœ“ Result: 61/152 correct = 40.13%\n",
            "\n",
            "======================================================================\n",
            "EVALUATION SUMMARY\n",
            "======================================================================\n",
            "Model: allenai/OLMo-2-0425-1B\n",
            "None (full precision)\n",
            "Total Subjects: 1\n",
            "Total Questions: 152\n",
            "Total Correct: 61\n",
            "Overall Accuracy: 40.13%\n",
            "Duration: 0.3 minutes\n",
            "======================================================================\n",
            "\n",
            "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260129_205154.json\n",
            "\n",
            "ðŸ“Š Top 5 Subjects:\n",
            "  1. astronomy: 40.13%\n",
            "\n",
            "ðŸ“‰ Bottom 5 Subjects:\n",
            "  1. astronomy: 40.13%\n",
            "\n",
            "======================================================================\n",
            "ðŸ’¾ To download results in Colab:\n",
            "======================================================================\n",
            "from google.colab import files\n",
            "files.download('llama_3.2_1b_mmlu_results_full_20260129_205154.json')\n",
            "\n",
            "âœ… Evaluation complete!\n",
            "\n",
            "real\t2m4.327s\n",
            "user\t1m23.910s\n",
            "sys\t0m26.681s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhzpZUBqZYw3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}